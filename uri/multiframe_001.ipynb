{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastai import *\n",
    "from fastai.vision import *\n",
    "from fastai.callbacks import *\n",
    "from superres import superres_metrics, ssim, psnr\n",
    "from superres import RRDB_Net, MultiImageToMultiChannel, TwoXModel, TwoYLoss\n",
    "import czifile\n",
    "from superres.helpers import get_czi_shape_info, build_index\n",
    "import imageio\n",
    "\n",
    "datasetname = 'multiframe_001'\n",
    "data_path = Path('/scratch/bpho')\n",
    "datasets = data_path/'datasets'\n",
    "datasources = data_path/'datasources'\n",
    "dataset = datasets/datasetname\n",
    "\n",
    "hr_path = dataset/'hr'\n",
    "lr_path = dataset/'lr'\n",
    "lr_up_path = dataset/'lr_up'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NpyRawImageList(ImageList):\n",
    "    def open(self, fn):\n",
    "        img_data = np.load(fn)\n",
    "        return Image(tensor(img_data[None]))\n",
    "    \n",
    "    def analyze_pred(self, pred:Tensor):\n",
    "        if len(pred.shape) > 3: pred = pred[0]\n",
    "        return pred\n",
    "    \n",
    "    def reconstruct(self, t: torch.Tensor):\n",
    "        return Image(t.float().clamp(min=0,max=1))\n",
    "\n",
    "\n",
    "class TransformableLists(ItemBase):\n",
    "    def __init__(self, img_lists):\n",
    "        self.img_lists = img_lists\n",
    "       \n",
    "    def __repr__(self):\n",
    "        return f'MultiImg: {len(self.img_lists)}'\n",
    "    \n",
    "    @property\n",
    "    def data(self):\n",
    "        img_data = torch.stack([torch.stack([img.data for img in img_list]) \n",
    "                                            for img_list in self.img_lists])\n",
    "        data = tensor(img_data)\n",
    "        return data\n",
    "    \n",
    "    def apply_tfms(self, tfms, **kwargs):\n",
    "        first_time = True\n",
    "        save_img_lists = []\n",
    "        for img_list in self.img_lists:\n",
    "            save_img_list = []\n",
    "            for img in img_list:\n",
    "                new_img = img.apply_tfms(tfms, do_resolve=first_time, **kwargs)\n",
    "                first_time = False\n",
    "                save_img_list.append(new_img)\n",
    "            save_img_lists.append(save_img_list)\n",
    "        self.img_lists = save_img_lists\n",
    "        return self\n",
    "    \n",
    "class MultiImageDataBunch(ImageDataBunch):\n",
    "    def batch_stats(self, funcs:Collection[Callable]=None)->Tensor:\n",
    "        \"Grab a batch of data and call reduction function `func` per channel\"\n",
    "        funcs = ifnone(funcs, [torch.mean,torch.std])\n",
    "        ds_type = DatasetType.Valid if self.valid_dl else DatasetType.Train\n",
    "        x = self.one_batch(ds_type=ds_type, denorm=False)[0].cpu()\n",
    "        def multi_channel_view(x):\n",
    "            return x.transpose(3,0).contiguous().view(x.shape[3],-1)\n",
    "        return [func(multi_channel_view(x), 1) for func in funcs]\n",
    "\n",
    "    def normalize(self, stats:Collection[Tensor]=None, do_x:bool=True, do_y:bool=False)->None:\n",
    "        \"Add normalize transform using `stats` (defaults to `DataBunch.batch_stats`)\"\n",
    "        if getattr(self,'norm',False): raise Exception('Can not call normalize twice')\n",
    "        if stats is None: self.stats = self.batch_stats()\n",
    "        else:             self.stats = stats\n",
    "        self.norm,self.denorm = normalize_funcs(*self.stats, do_x=do_x, do_y=do_y)\n",
    "        self.add_tfm(self.norm)\n",
    "        return self\n",
    "    \n",
    "class MultiImageList(ItemList):\n",
    "    _bunch = MultiImageDataBunch\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        \n",
    "    def open(self, fn):\n",
    "        img_data = np.load(fn)\n",
    "        img_lists = []\n",
    "        for j in range(img_data.shape[0]):\n",
    "            imgs = []\n",
    "            for i in range(img_data.shape[1]):\n",
    "                imgs.append(Image(tensor(img_data[j,i,:,:][None])))\n",
    "            img_lists.append(imgs)\n",
    "        return TransformableLists(img_lists)\n",
    "    \n",
    "    def get(self, i):\n",
    "        fn = super().get(i)\n",
    "        img_lists = self.open(fn)\n",
    "        return img_lists    \n",
    "    \n",
    "\n",
    "class MultiToMultiImageList(MultiImageList):\n",
    "    _label_cls = NpyRawImageList"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_to_hr(x):\n",
    "    hr_name = x.relative_to(lr_path)\n",
    "    return hr_path/hr_name\n",
    "\n",
    "def get_src(size=128, scale=1):\n",
    "    if scale != 1: use_lr_path = lr_path\n",
    "    else: use_lr_path = lr_up_path\n",
    "        \n",
    "    def map_to_hr(x):\n",
    "        hr_name = x.relative_to(use_lr_path)\n",
    "        return hr_path/hr_name\n",
    "    \n",
    "    src = (MultiToMultiImageList.from_folder(use_lr_path, extensions=['.npy'])\n",
    "           .split_by_folder()\n",
    "           .label_from_func(map_to_hr))\n",
    "    return src\n",
    "\n",
    "def get_data(bs, size, scale=1, max_zoom=1.):\n",
    "    src = get_src(size, scale=scale)    \n",
    "    tfms = get_transforms(flip_vert=True, max_zoom=max_zoom)\n",
    "    data = (src\n",
    "            .transform(tfms, size=size)\n",
    "            .transform_y(tfms, size=size*scale)\n",
    "            .databunch(bs=bs).normalize(do_y=True))\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('cool')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bs = 8\n",
    "size = 128\n",
    "data = get_data(bs, size, scale=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bs = 4\n",
    "size = 256\n",
    "data = get_data(bs, size, scale=4)\n",
    "\n",
    "in_c = 3\n",
    "out_c = 1\n",
    "nf = gcval = 16\n",
    "nb = 5\n",
    "loss = TwoYLoss(stable_wt=0.0)\n",
    "model = TwoXModel(MultiImageToMultiChannel(RRDB_Net(3, 1, nf, nb, gc=gcval )))\n",
    "learn = Learner(data, model, callback_fns=LossMetrics, loss_func=loss)\n",
    "#learn = learn.load('rddb.1')\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.lr_find()\n",
    "learn.recorder.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.fit_one_cycle(3, max_lr=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.save('rddb.1')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "movie_files = list(Path('/scratch/bpho/datasets/movies_001/test').glob('*.czi'))\n",
    "#movie_files += list(Path('/scratch/bpho/datasources/low_res_test/').glob('low res confocal*.czi'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def image_from_tiles(learn, in_img, tile_sz=128, scale=4):\n",
    "    cur_size = in_img.shape[1:3]\n",
    "    c = in_img.shape[0]\n",
    "    new_size = (cur_size[0]*scale, cur_size[1]*scale)\n",
    "    w, h = cur_size\n",
    "    \n",
    "    in_tile = torch.zeros((c,tile_sz,tile_sz))\n",
    "    out_img = torch.zeros((1,w*scale,h*scale))\n",
    "    \n",
    "    for x_tile in range(math.ceil(w/tile_sz)):\n",
    "        for y_tile in range(math.ceil(h/tile_sz)):\n",
    "            x_start = x_tile\n",
    "\n",
    "            x_start = x_tile*tile_sz\n",
    "            x_end = min(x_start+tile_sz, w)\n",
    "            y_start = y_tile*tile_sz\n",
    "            y_end = min(y_start+tile_sz, h)\n",
    "            \n",
    "            \n",
    "            in_tile[:,0:(x_end-x_start), 0:(y_end-y_start)] = tensor(in_img[:,x_start:x_end, y_start:y_end])\n",
    "            \n",
    "            img_list = [Image(in_tile[i][None]) for i in range(3)]\n",
    "            tlist = TransformableLists([img_list, img_list])\n",
    "            out_tile,_,_ = learn.predict(tlist)\n",
    "            \n",
    "            out_x_start = x_start * scale\n",
    "            out_x_end = x_end * scale\n",
    "            out_y_start = y_start * scale\n",
    "            out_y_end = y_end * scale\n",
    "\n",
    "            #print(\"out: \", out_x_start, out_y_start, \",\", out_x_end, out_y_end)\n",
    "            in_x_start = 0\n",
    "            in_y_start = 0\n",
    "            in_x_end = (x_end-x_start) * scale\n",
    "            in_y_end = (y_end-y_start) * scale\n",
    "            #print(\"tile: \",in_x_start, in_y_start, \",\", in_x_end, in_y_end)\n",
    "           \n",
    "            out_img[:,out_x_start:out_x_end, out_y_start:out_y_end] = out_tile.data[:,\n",
    "                                                                                  in_x_start:in_x_end, \n",
    "                                                                                  in_y_start:in_y_end]\n",
    "    return out_img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def czi_predict_movie(learn, czi_in, orig_out='orig.tif', pred_out='pred.tif', size=128, wsize=3):\n",
    "    with czifile.CziFile(czi_in) as czi_f:\n",
    "        proc_axes, proc_shape = get_czi_shape_info(czi_f)\n",
    "        channels = proc_shape['C']\n",
    "        depths = proc_shape['Z']\n",
    "        times = proc_shape['T']\n",
    "        x,y = proc_shape['X'], proc_shape['Y']\n",
    "        \n",
    "        \n",
    "        data = czi_f.asarray()\n",
    "        preds = []\n",
    "        origs = []\n",
    "        img_max = None\n",
    "        for t in progress_bar(list(range(0,times-wsize+1))):\n",
    "            idx = build_index(proc_axes, {'T': slice(t,t+wsize), 'C': 0, 'Z':0, 'X':slice(0,x),'Y':slice(0,y)})\n",
    "            img = data[idx].astype(np.float32)\n",
    "            img /= img.max()\n",
    "            \n",
    "            out_img = image_from_tiles(learn, img, tile_sz=size)\n",
    "            pred = (out_img*255).cpu().numpy().astype(np.uint8)\n",
    "            preds.append(pred)\n",
    "            orig = (img[1][None]*255).astype(np.uint8)\n",
    "            origs.append(orig)\n",
    "            \n",
    "        all_y = np.concatenate(preds)\n",
    "        #print(all_y.shape)\n",
    "        imageio.mimwrite(pred_out, all_y) #, fps=30, macro_block_size=None) # for mp4\n",
    "        all_y = np.concatenate(origs)\n",
    "        #print(all_y.shape)\n",
    "        imageio.mimwrite(orig_out, all_y) #, fps=30, macro_block_size=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "bs = 1\n",
    "size = 512\n",
    "data = get_data(bs, size, scale=4)\n",
    "\n",
    "loss = TwoYLoss()\n",
    "model = TwoXModel(MultiImageToMultiChannel(RRDB_Net(3, 1, nf, nb, gc=gcval )))\n",
    "learn = Learner(data, model, callback_fns=LossMetrics, loss_func=loss)\n",
    "gc.collect()\n",
    "learn = learn.load('rddb.1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "        <style>\n",
       "            /* Turns off some styling */\n",
       "            progress {\n",
       "                /* gets rid of default border in Firefox and Opera. */\n",
       "                border: none;\n",
       "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "                background-size: auto;\n",
       "            }\n",
       "            .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "                background: #F44336;\n",
       "            }\n",
       "        </style>\n",
       "      <progress value='298' class='' max='298', style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      100.00% [298/298 00:53<00:00]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for fn in movie_files:\n",
    "    pred_name = f'{fn.stem}_pred.tif'\n",
    "    orig_name = f'{fn.stem}_orig.tif'\n",
    "    czi_predict_movie(learn, fn, size=size, orig_out=orig_name, pred_out=pred_name )\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
